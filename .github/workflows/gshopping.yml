name: Google Shopping Scraper

on:
  workflow_dispatch:
    inputs:
      input_filename:
        description: "Input CSV filename on FTP"
        required: true
        default: "google_shopping.csv"
        type: string
      total_chunks:
        description: "Number of chunks to split into"
        required: true
        default: "4"
        type: string

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - id: matrix
        run: |
          TOTAL_CHUNKS=${{ github.event.inputs.total_chunks || 4 }}

          MATRIX="["
          for ((i=1;i<=TOTAL_CHUNKS;i++)); do
            MATRIX+="{\"chunk_id\":$i},"
          done
          MATRIX="${MATRIX%,}]"

          echo "matrix=$MATRIX" >> "$GITHUB_OUTPUT"
          echo "Matrix: $MATRIX"

  scrape:
    needs: plan
    runs-on: ubuntu-22.04
    timeout-minutes: 180  # Increased timeout for captcha handling

    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: 'pip'

      - name: Install Chrome and dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            wget \
            gnupg \
            ca-certificates \
            unzip \
            xvfb \
            ffmpeg \
            libxss1 \
            libnss3 \
            libnspr4 \
            libatk-bridge2.0-0 \
            libgtk-3-0 \
            libx11-xcb1 \
            libxcomposite1 \
            libxrandr2 \
            libgbm1 \
            libasound2 \
            fonts-liberation \
            libu2f-udev \
            pulseaudio \
            pulseaudio-utils \
            alsa-utils \
            libappindicator3-1 \
            libindicator3-7 \
            libappindicator1 \
            libindicator7
          
          # Download and install Chrome
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          
          # Verify Chrome installation
          google-chrome-stable --version

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install \
            selenium==4.18.1 \
            undetected-chromedriver==3.5.5 \
            webdriver-manager==4.0.1 \
            pandas==2.1.4 \
            beautifulsoup4==4.12.2 \
            requests==2.31.0 \
            lxml==4.9.3 \
            fake-useragent==1.4.0 \
            python-dateutil==2.8.2 \
            SpeechRecognition==3.10.1 \
            pydub==0.25.1 \
            chromedriver-autoinstaller==0.6.4 \
            ftplib==1.0.1  # ADDED: Explicit FTP support

      - name: Setup audio virtual sink (for pulseaudio)
        run: |
          # Start pulseaudio with a virtual sink
          pulseaudio --start --exit-idle-time=-1
          pactl load-module module-virtual-sink sink_name=v1
          pactl set-default-sink v1
          pactl set-default-source v1.monitor
          echo "Audio virtual sink configured"

      - name: Run scraper
        env:
          DISPLAY: :99
          PULSE_SERVER: unix:/run/user/$(id -u)/pulse/native  # ADDED: PulseAudio server
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_PORT: ${{ secrets.FTP_PORT }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_PATH: ${{ secrets.FTP_PATH }}
        run: |
          # Start Xvfb with larger screen and audio support
          Xvfb :99 -screen 0 1920x1080x24 -ac &
          export DISPLAY=:99
          
          # Wait for Xvfb to start
          sleep 5
          
          # Start a simple window manager to handle popups better
          sudo apt-get install -y fluxbox
          fluxbox &
          
          INPUT_FILE="${{ github.event.inputs.input_filename || 'google_shopping.csv' }}"
          TOTAL_CHUNKS="${{ github.event.inputs.total_chunks || 4 }}"

          # Run with retry logic for the whole script
          MAX_RETRIES=3
          RETRY_COUNT=0
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            echo "Attempt $((RETRY_COUNT+1)) of $MAX_RETRIES"
            
            python -u gshopping/gscrapperci.py \
              --chunk-id ${{ matrix.chunk_id }} \
              --total-chunks $TOTAL_CHUNKS \
              --input-file "$INPUT_FILE"
            
            EXIT_CODE=$?
            
            if [ $EXIT_CODE -eq 0 ]; then
              echo "âœ“ Scraping completed successfully"
              break
            else
              echo "âœ— Scraping failed with exit code $EXIT_CODE"
              RETRY_COUNT=$((RETRY_COUNT+1))
              
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "Retrying in 60 seconds..."
                sleep 60
                
                # Clean up any lingering processes
                pkill -f chrome || true
                pkill -f Xvfb || true
                
                # Restart Xvfb
                Xvfb :99 -screen 0 1920x1080x24 -ac &
                sleep 5
              fi
            fi
          done
          
          # Exit with the last exit code if all retries failed
          if [ $RETRY_COUNT -eq $MAX_RETRIES ] && [ $EXIT_CODE -ne 0 ]; then
            echo "âœ— All retry attempts failed"
            exit $EXIT_CODE
          fi

      - name: Upload chunk results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: chunk-${{ matrix.chunk_id }}-${{ github.run_id }}
          path: output/
          retention-days: 1

  merge:
    needs: scrape
    runs-on: ubuntu-latest
    if: always()  # Run even if some chunks failed

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install pandas
        run: |
          python -m pip install --upgrade pip
          pip install pandas==2.1.4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: chunks
          pattern: chunk-*
          merge-multiple: true

      - name: Merge CSV files
        run: |
          cat > merge_results.py <<'EOF'
          import os
          import pandas as pd
          from datetime import datetime
          import glob

          # Find all CSV files
          product_files = glob.glob("chunks/**/product_info_*.csv", recursive=True)
          seller_files = glob.glob("chunks/**/seller_info_*.csv", recursive=True)

          print(f"Found {len(product_files)} product files")
          print(f"Found {len(seller_files)} seller files")

          ts = datetime.now().strftime("%Y%m%d_%H%M%S")

          # Merge product files
          if product_files:
              dfs = []
              for f in product_files:
                  try:
                      df = pd.read_csv(f)
                      dfs.append(df)
                      print(f"Loaded {f}: {len(df)} rows")
                  except Exception as e:
                      print(f"Error loading {f}: {e}")
              
              if dfs:
                  df_merged = pd.concat(dfs, ignore_index=True)
                  df_merged.sort_values("product_id", inplace=True)
                  
                  # Remove duplicates if any
                  df_merged = df_merged.drop_duplicates(subset=['product_id'], keep='last')
                  
                  output_file = f"merged_products_{ts}.csv"
                  df_merged.to_csv(output_file, index=False)
                  print(f"âœ“ Saved {output_file} with {len(df_merged)} rows")

          # Merge seller files
          if seller_files:
              dfs = []
              for f in seller_files:
                  try:
                      df = pd.read_csv(f)
                      dfs.append(df)
                      print(f"Loaded {f}: {len(df)} rows")
                  except Exception as e:
                      print(f"Error loading {f}: {e}")
              
              if dfs:
                  df_merged = pd.concat(dfs, ignore_index=True)
                  df_merged.sort_values(["product_id", "seller"], inplace=True)
                  
                  # Remove duplicates if any
                  df_merged = df_merged.drop_duplicates(subset=['product_id', 'seller', 'seller_url'], keep='last')
                  
                  output_file = f"merged_sellers_{ts}.csv"
                  df_merged.to_csv(output_file, index=False)
                  print(f"âœ“ Saved {output_file} with {len(df_merged)} rows")
          EOF

          python merge_results.py

      - name: Upload merged files to FTP
        if: success()  # Only upload if merge was successful
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_PATH: ${{ secrets.FTP_PATH }}
        run: |
          # Create FTP upload script
          cat > ftp_upload.sh <<'EOF'
          #!/bin/bash
          HOST="$1"
          USER="$2"
          PASS="$3"
          PATH="$4"
          FILE="$5"

          ftp -n $HOST <<END_SCRIPT
          quote USER $USER
          quote PASS $PASS
          binary
          cd $PATH
          put $FILE
          quit
          END_SCRIPT
          EOF

          chmod +x ftp_upload.sh

          # Upload each merged file
          for file in merged_products_*.csv merged_sellers_*.csv; do
            if [ -f "$file" ]; then
              echo "Uploading $file..."
              ./ftp_upload.sh "$FTP_HOST" "$FTP_USER" "$FTP_PASS" "$FTP_PATH" "$file"
              
              if [ $? -eq 0 ]; then
                echo "âœ“ Uploaded $file"
              else
                echo "âœ— Failed to upload $file"
              fi
            fi
          done

      - name: Upload merged artifacts
        uses: actions/upload-artifact@v4
        with:
          name: merged-results-${{ github.run_id }}
          path: |
            merged_products_*.csv
            merged_sellers_*.csv
          retention-days: 7

  notify:
    needs: [merge]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Summary
        run: |
          echo "## Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Workflow:** ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Run ID:** ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ needs.merge.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ”— [View Run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY