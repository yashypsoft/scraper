name: Google Shopping Scraper

on:
  workflow_dispatch:
    inputs:
      input_filename:
        description: "Input CSV filename on FTP"
        required: true
        default: "google_shopping.csv"
        type: string
      total_chunks:
        description: "Number of chunks per round"
        required: true
        default: "20"
        type: string
      run_depth:
        description: "Current recursive run depth"
        required: true
        default: "1"
        type: string
      max_depth:
        description: "Maximum recursive run depth"
        required: true
        default: "10"
        type: string

permissions:
  contents: read
  actions: write

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - id: matrix
        run: |
          TOTAL_CHUNKS=${{ github.event.inputs.total_chunks || 20 }}

          MATRIX="["
          for ((i=1;i<=TOTAL_CHUNKS;i++)); do
            MATRIX+="{\"chunk_id\":$i},"
          done
          MATRIX="${MATRIX%,}]"

          echo "matrix=$MATRIX" >> "$GITHUB_OUTPUT"
          echo "Matrix: $MATRIX"

  scrape:
    needs: plan
    runs-on: ubuntu-22.04
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install Chrome and dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            wget \
            gnupg \
            ca-certificates \
            unzip \
            xvfb \
            libxss1 \
            libappindicator3-1 \
            libindicator7 \
            libnss3 \
            libnspr4 \
            libatk-bridge2.0-0 \
            libgtk-3-0 \
            libx11-xcb1 \
            libxcomposite1 \
            libxrandr2 \
            libgbm1 \
            libasound2 \
            fonts-liberation \
            libu2f-udev

          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

          google-chrome-stable --version

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install \
            selenium==4.18.1 \
            undetected-chromedriver==3.5.5 \
            webdriver-manager \
            pandas==2.1.4 \
            beautifulsoup4==4.12.2 \
            requests==2.31.0 \
            lxml==4.9.3 \
            fake-useragent==1.4.0 \
            python-dateutil==2.8.2 \
            speechrecognition==3.10.1 \
            pydub==0.25.1 \
            chromedriver-autoinstaller==0.6.4

      - name: Run scraper chunk
        env:
          DISPLAY: :99
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_PORT: ${{ secrets.FTP_PORT }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_PATH: ${{ secrets.FTP_PATH }}
        run: |
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 3

          INPUT_FILE="${{ github.event.inputs.input_filename || 'google_shopping.csv' }}"
          TOTAL_CHUNKS="${{ github.event.inputs.total_chunks || 20 }}"

          python -u gshopping/gscrapperci.py \
            --chunk-id ${{ matrix.chunk_id }} \
            --total-chunks "$TOTAL_CHUNKS" \
            --input-file "$INPUT_FILE"

      - name: Upload chunk results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: chunk-${{ matrix.chunk_id }}
          path: output/
          retention-days: 2

  merge:
    needs: scrape
    runs-on: ubuntu-latest
    outputs:
      has_remaining: ${{ steps.merge_results.outputs.has_remaining }}
      next_input_filename: ${{ steps.merge_results.outputs.next_input_filename }}
      final_remaining_file: ${{ steps.merge_results.outputs.final_remaining_file }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install pandas
        run: |
          python -m pip install --upgrade pip
          pip install pandas==2.1.4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: chunks

      - name: Merge round files and update cumulative FTP files
        id: merge_results
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_PORT: ${{ secrets.FTP_PORT }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_PATH: ${{ secrets.FTP_PATH }}
          RUN_DEPTH: ${{ github.event.inputs.run_depth || '1' }}
          MAX_DEPTH: ${{ github.event.inputs.max_depth || '10' }}
        run: |
          python - <<'PY'
          import os
          import ftplib
          import pandas as pd
          from datetime import datetime

          ftp_host = os.environ.get("FTP_HOST")
          ftp_user = os.environ.get("FTP_USER")
          ftp_pass = os.environ.get("FTP_PASS")
          ftp_port = int(os.environ.get("FTP_PORT", "21"))
          ftp_path = os.environ.get("FTP_PATH", "/scrap/")
          run_depth = int(os.environ.get("RUN_DEPTH", "1"))
          max_depth = int(os.environ.get("MAX_DEPTH", "10"))
          PRODUCT_FINAL_COLUMNS = [
              "product_id",
              "web_id",
              "name",
              "mpn_sku",
              "gtin",
              "brand",
              "category",
              "keyword",
              "url",
              "osb_url",
              "last_response",
              "osb_url_match",
              "product_url",
              "seller",
              "product_name",
              "cid",
              "pid",
              "last_fetched_date",
              "osb_position",
              "osb_id",
              "seller_count",
              "status",
          ]

          product_files = []
          seller_files = []
          remaining_files = []

          for root, _, files in os.walk("chunks"):
              for f in files:
                  p = os.path.join(root, f)
                  if f.endswith(".csv") and "product_info_" in f and "chunk" in f:
                      product_files.append(p)
                  elif f.endswith(".csv") and "seller_info_" in f and "chunk" in f:
                      seller_files.append(p)
                  elif f.endswith(".csv") and "gshopping_remaining_" in f and "chunk" in f:
                      remaining_files.append(p)

          def merge_files(paths, out_name, sort_cols=None, expected_cols=None):
              frames = []
              for p in sorted(paths):
                  try:
                      df = pd.read_csv(p)
                      if not df.empty:
                          frames.append(df)
                  except Exception as e:
                      print(f"Warning reading {p}: {e}")
              if not frames:
                  return None, None
              df = pd.concat(frames, ignore_index=True)
              if expected_cols:
                  for col in expected_cols:
                      if col not in df.columns:
                          df[col] = ""
                  df = df.loc[:, expected_cols]
              if sort_cols:
                  cols = [c for c in sort_cols if c in df.columns]
                  if cols:
                      df = df.sort_values(cols)
              df.to_csv(out_name, index=False)
              return out_name, df

          def normalize_product_columns(df):
              if df is None:
                  return None
              for col in PRODUCT_FINAL_COLUMNS:
                  if col not in df.columns:
                      df[col] = ""
              return df.loc[:, PRODUCT_FINAL_COLUMNS]

          round_products_file, round_products_df = merge_files(
              product_files, "round_products.csv", ["product_id"], expected_cols=PRODUCT_FINAL_COLUMNS
          )
          round_sellers_file, round_sellers_df = merge_files(seller_files, "round_sellers.csv", ["product_id", "seller"])
          round_remaining_file, round_remaining_df = merge_files(remaining_files, "gshopping_remaining.csv", ["product_id"])

          if not all([ftp_host, ftp_user, ftp_pass]):
              raise RuntimeError("FTP credentials are missing")

          ftp = ftplib.FTP()
          ftp.connect(ftp_host, ftp_port)
          ftp.login(ftp_user, ftp_pass)
          ftp.set_pasv(True)

          if ftp_path and ftp_path != '/':
              parts = [p for p in ftp_path.strip('/').split('/') if p]
              cur = ''
              for d in parts:
                  cur += '/' + d
                  try:
                      ftp.cwd(cur)
                  except Exception:
                      ftp.mkd(cur)
                      ftp.cwd(cur)

          base_cwd = ftp.pwd()

          def ftp_download_if_exists(remote_name, local_name):
              try:
                  with open(local_name, 'wb') as f:
                      ftp.retrbinary(f"RETR {remote_name}", f.write)
                  return True
              except Exception:
                  if os.path.exists(local_name):
                      os.remove(local_name)
                  return False

          def ftp_upload(local_name, remote_name, subdir=None):
              ftp.cwd(base_cwd)
              if subdir:
                  for d in [p for p in subdir.strip("/").split("/") if p]:
                      try:
                          ftp.cwd(d)
                      except Exception:
                          ftp.mkd(d)
                          ftp.cwd(d)
              with open(local_name, 'rb') as f:
                  ftp.storbinary(f"STOR {remote_name}", f)
              ftp.cwd(base_cwd)
              print(f"Uploaded {remote_name}" + (f" to {subdir}" if subdir else ""))

          def ftp_delete_if_exists(remote_name):
              try:
                  ftp.delete(remote_name)
                  print(f"Deleted {remote_name}")
              except Exception:
                  pass

          unresolved_ids = set()
          if round_remaining_df is not None and "product_id" in round_remaining_df.columns:
              unresolved_ids = set(round_remaining_df["product_id"].astype(str).str.strip().tolist())

          if round_products_df is not None:
              if "status" in round_products_df.columns:
                  round_products_df = round_products_df[
                      round_products_df["status"].astype(str).str.strip().str.lower() != "captcha_failed"
                  ]
              if unresolved_ids and "product_id" in round_products_df.columns:
                  round_products_df = round_products_df[
                      ~round_products_df["product_id"].astype(str).str.strip().isin(unresolved_ids)
                  ]

          if round_sellers_df is not None and unresolved_ids and "product_id" in round_sellers_df.columns:
              round_sellers_df = round_sellers_df[
                  ~round_sellers_df["product_id"].astype(str).str.strip().isin(unresolved_ids)
              ]

          # Start fresh for a new manual run (depth 1): do not append to prior run totals.
          if run_depth == 1:
              ftp_delete_if_exists("gshopping_products_final.csv")
              ftp_delete_if_exists("gshopping_sellers_final.csv")

          def load_prev_csv(remote_name):
              if not ftp_download_if_exists(remote_name, "_prev.csv"):
                  return None
              try:
                  return pd.read_csv("_prev.csv")
              except Exception:
                  return None

          prev_products_df = load_prev_csv("gshopping_products_final.csv")
          prev_sellers_df = load_prev_csv("gshopping_sellers_final.csv")
          round_products_df = normalize_product_columns(round_products_df)
          prev_products_df = normalize_product_columns(prev_products_df)

          if prev_products_df is not None and unresolved_ids and "product_id" in prev_products_df.columns:
              prev_products_df = prev_products_df[
                  ~prev_products_df["product_id"].astype(str).str.strip().isin(unresolved_ids)
              ]
          if prev_sellers_df is not None and unresolved_ids and "product_id" in prev_sellers_df.columns:
              prev_sellers_df = prev_sellers_df[
                  ~prev_sellers_df["product_id"].astype(str).str.strip().isin(unresolved_ids)
              ]

          product_frames = [df for df in [prev_products_df, round_products_df] if df is not None and not df.empty]
          seller_frames = [df for df in [prev_sellers_df, round_sellers_df] if df is not None and not df.empty]
          final_subdir = f"gshoping/{datetime.utcnow().strftime('%Y-%m-%d')}"

          if product_frames:
              final_products_df = pd.concat(product_frames, ignore_index=True)
              final_products_df = normalize_product_columns(final_products_df)
              if "product_id" in final_products_df.columns:
                  final_products_df["__pid"] = final_products_df["product_id"].astype(str).str.strip()
                  final_products_df = final_products_df.drop_duplicates(subset=["__pid"], keep="last").drop(columns=["__pid"])
                  final_products_df = final_products_df.sort_values(["product_id"])
              final_products_df.to_csv("gshopping_products_final.csv", index=False)
              ftp_upload("gshopping_products_final.csv", "gshopping_products_final.csv", subdir=final_subdir)
              ftp_upload("gshopping_products_final.csv", "gshopping_products_final.csv")

          if seller_frames:
              final_sellers_df = pd.concat(seller_frames, ignore_index=True)
              dedupe_cols = [c for c in ["product_id", "seller", "seller_product_name", "seller_url", "seller_price"] if c in final_sellers_df.columns]
              if dedupe_cols:
                  final_sellers_df = final_sellers_df.drop_duplicates(subset=dedupe_cols, keep="last")
              sort_cols = [c for c in ["product_id", "seller"] if c in final_sellers_df.columns]
              if sort_cols:
                  final_sellers_df = final_sellers_df.sort_values(sort_cols)
              final_sellers_df.to_csv("gshopping_sellers_final.csv", index=False)
              ftp_upload("gshopping_sellers_final.csv", "gshopping_sellers_final.csv", subdir=final_subdir)
              ftp_upload("gshopping_sellers_final.csv", "gshopping_sellers_final.csv")

          has_remaining = "false"
          next_input_filename = ""
          final_remaining_file = ""

          if round_remaining_file and os.path.exists(round_remaining_file):
              rem_df = pd.read_csv(round_remaining_file)
              if not rem_df.empty:
                  has_remaining = "true"
                  next_input_filename = f"gshopping_remaining_depth_{run_depth}.csv"
                  ftp_upload(round_remaining_file, next_input_filename)

                  # Keep a stable remaining filename only when recursion stops at max depth.
                  if run_depth >= max_depth:
                      ftp_upload(round_remaining_file, "gshopping_remaining.csv")
                      final_remaining_file = "gshopping_remaining.csv"
              else:
                  ftp_delete_if_exists("gshopping_remaining.csv")
          else:
              ftp_delete_if_exists("gshopping_remaining.csv")

          ftp.quit()

          with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
              f.write(f"has_remaining={has_remaining}\n")
              f.write(f"next_input_filename={next_input_filename}\n")
              f.write(f"final_remaining_file={final_remaining_file}\n")

          print(f"has_remaining={has_remaining}")
          print(f"next_input_filename={next_input_filename}")
          print(f"final_remaining_file={final_remaining_file}")
          PY

      - name: Upload merged artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: merged-round-depth-${{ github.event.inputs.run_depth || '1' }}
          path: |
            round_products.csv
            round_sellers.csv
            gshopping_remaining.csv
            gshopping_products_final.csv
            gshopping_sellers_final.csv
          retention-days: 7

  dispatch_next_round:
    needs: merge
    if: ${{ needs.merge.outputs.has_remaining == 'true' && fromJSON(github.event.inputs.run_depth || '1') < fromJSON(github.event.inputs.max_depth || '10') }}
    runs-on: ubuntu-latest

    steps:
      - name: Trigger next round workflow
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const runDepth = Number('${{ github.event.inputs.run_depth || '1' }}');
            const nextDepth = runDepth + 1;
            const totalChunks = '${{ github.event.inputs.total_chunks || '20' }}';
            const maxDepth = '${{ github.event.inputs.max_depth || '10' }}';
            const nextInput = '${{ needs.merge.outputs.next_input_filename }}';

            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'gshopping.yml',
              ref: 'refs/heads/main',
              inputs: {
                input_filename: nextInput,
                total_chunks: totalChunks,
                run_depth: String(nextDepth),
                max_depth: maxDepth,
              },
            });

            core.info(`Triggered next round depth=${nextDepth} with input=${nextInput}`);

  cleanup_depth_files:
    needs: [merge, dispatch_next_round]
    if: ${{ always() && needs.dispatch_next_round.result == 'skipped' }}
    runs-on: ubuntu-latest

    steps:
      - name: Cleanup depth files on FTP
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_PORT: ${{ secrets.FTP_PORT }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_PATH: ${{ secrets.FTP_PATH }}
        run: |
          python - <<'PY'
          import os
          import ftplib

          ftp_host = os.environ.get("FTP_HOST")
          ftp_user = os.environ.get("FTP_USER")
          ftp_pass = os.environ.get("FTP_PASS")
          ftp_port = int(os.environ.get("FTP_PORT", "21"))
          ftp_path = os.environ.get("FTP_PATH", "/scrap/")

          if not all([ftp_host, ftp_user, ftp_pass]):
              raise RuntimeError("FTP credentials are missing")

          ftp = ftplib.FTP()
          ftp.connect(ftp_host, ftp_port)
          ftp.login(ftp_user, ftp_pass)
          ftp.set_pasv(True)

          if ftp_path and ftp_path != '/':
              parts = [p for p in ftp_path.strip('/').split('/') if p]
              cur = ''
              for d in parts:
                  cur += '/' + d
                  try:
                      ftp.cwd(cur)
                  except Exception:
                      ftp.mkd(cur)
                      ftp.cwd(cur)

          try:
              names = ftp.nlst()
          except Exception:
              names = []

          for name in names:
              if name.startswith("gshopping_remaining_depth_") and name.endswith(".csv"):
                  try:
                      ftp.delete(name)
                      print(f"Deleted {name}")
                  except Exception as e:
                      print(f"Could not delete {name}: {e}")

          ftp.quit()
          PY

  notify:
    needs: [merge, dispatch_next_round, cleanup_depth_files]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Summary
        run: |
          echo "Workflow finished"
          echo "Run depth: ${{ github.event.inputs.run_depth || '1' }}"
          echo "Has remaining: ${{ needs.merge.outputs.has_remaining }}"
          echo "Next input: ${{ needs.merge.outputs.next_input_filename }}"
          echo "Run: https://github.com/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID"
