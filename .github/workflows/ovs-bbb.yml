name: Parallel Sitemap Scraper OVS/BBB (Enhanced v1.3)

on:
  workflow_dispatch:
    inputs:
      url:
        description: "Base site URL"
        required: true
        default: "https://www.graysonliving.com"
      api_url:
        description: "API base URL"
        required: true
        default: "https://www.graysonliving.com/api/products"
      total_sitemaps:
        description: "Total sitemaps to process (0 = auto)"
        default: "0"
      sitemaps_per_job:
        description: "Sitemaps per parallel job"
        default: "2"
      urls_per_sitemap:
        description: "Max URLs per sitemap"
        default: "200"
      max_workers:
        description: "Parallel requests per job"
        default: "8"
      request_delay:
        description: "Delay between requests (seconds)"
        default: "0.15"

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - id: matrix
        run: |
          TOTAL=${{ github.event.inputs.total_sitemaps }}
          PER_JOB=${{ github.event.inputs.sitemaps_per_job }}
          URL=${{ github.event.inputs.url }}
          API_URL=${{ github.event.inputs.api_url }}

          # If total is 0, try to detect from sitemap
          if [ "$TOTAL" -eq 0 ]; then
            echo "Detecting total sitemaps..."
            curl -s "$URL/sitemap.xml" | grep -c "<loc>" || TOTAL=20
          fi
          
          JOBS=$(( (TOTAL + PER_JOB - 1) / PER_JOB ))

          MATRIX="["
          for ((i=0;i<JOBS;i++)); do
            OFFSET=$(( i * PER_JOB ))
            MATRIX+="{\"offset\":$OFFSET,\"limit\":$PER_JOB,\"url\":\"$URL\",\"api_url\":\"$API_URL\"},"
          done
          MATRIX="${MATRIX%,}]"

          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
          echo "Jobs planned: $JOBS"

  scrape:
    needs: plan
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Run enhanced scraper
        env:
          CURR_URL: ${{ matrix.url }}
          API_BASE_URL: ${{ matrix.api_url }}
          SITEMAP_OFFSET: ${{ matrix.offset }}
          MAX_SITEMAPS: ${{ matrix.limit }}
          MAX_URLS_PER_SITEMAP: ${{ github.event.inputs.urls_per_sitemap }}
          MAX_WORKERS: ${{ github.event.inputs.max_workers }}
          REQUEST_DELAY: ${{ github.event.inputs.request_delay }}
        run: |
          python ovs-bbb/ovr.py

      - name: Upload chunk
        uses: actions/upload-artifact@v4
        with:
          name: chunk_${{ matrix.offset }}
          path: products_chunk_*.csv

  merge:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
      - uses: actions/download-artifact@v4
        with:
          path: chunks

      - name: Merge CSVs
        run: |
          echo "Merging CSV chunks..."
          # Find first CSV to get header
          FIRST=$(find chunks -name "products_chunk_*.csv" | head -n 1)
          
          if [ -z "$FIRST" ]; then
            echo "No CSV files found!"
            exit 1
          fi
          
          echo "Using header from: $FIRST"
          head -n 1 "$FIRST" > products_full.csv
          
          # Count total rows
          TOTAL_ROWS=0
          for f in chunks/*/*.csv; do
            if [ -f "$f" ]; then
              ROWS=$(tail -n +2 "$f" | wc -l)
              TOTAL_ROWS=$((TOTAL_ROWS + ROWS))
              tail -n +2 "$f" | sed '/^$/d' >> products_full.csv
            fi
          done
          
          FINAL_ROWS=$(tail -n +2 products_full.csv | wc -l)
          echo "Merged $FINAL_ROWS rows (expected: $TOTAL_ROWS)"

      - name: Build output filename
        id: meta
        run: |
          SITE=$(echo "${{ github.event.inputs.url }}" | sed -E 's|https?://||;s|/||g;s|\.[a-zA-Z]+$||')
          DATE=$(date +%F_%H%M%S)
          echo "name=${SITE}_${DATE}.csv" >> $GITHUB_OUTPUT

      - name: Upload to FTP
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_BASE_DIR: ${{ secrets.FTP_OUTPUT_PATH }}
          FILE: ${{ steps.meta.outputs.name }}
        run: |
          echo "Installing lftp..."
          sudo apt-get update && sudo apt-get install -y lftp
          
          echo "Uploading to FTP..."
          lftp -u "$FTP_USER","$FTP_PASS" "$FTP_HOST" <<EOF
          set ftp:ssl-allow no
          mkdir -p $FTP_BASE_DIR
          cd $FTP_BASE_DIR
          put products_full.csv -o $FILE
          bye
          EOF
          
          echo "Upload completed: $FILE"

      - name: Create summary
        run: |
          echo "=== Scraping Summary ==="
          echo "Site: ${{ github.event.inputs.url }}"
          echo "API: ${{ github.event.inputs.api_url }}"
          echo "Output file: ${{ steps.meta.outputs.name }}"
          echo "Total rows: $(tail -n +2 products_full.csv | wc -l)"
          echo "Chunks: $(ls -1 chunks/*/*.csv 2>/dev/null | wc -l)"
          echo "========================"