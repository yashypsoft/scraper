name: Parallel Sitemap Scraper (Cloudflare Resistant v2.0)

on:
  workflow_dispatch:
    inputs:
      url:
        description: "Base site URL (e.g., https://www.graysonliving.com)"
        required: true
        default: "https://www.graysonliving.com"
      total_sitemaps:
        description: "Total sitemaps to process (0 = auto)"
        default: "0"
      sitemaps_per_job:
        description: "Sitemaps per parallel job"
        default: "2"
      urls_per_sitemap:
        description: "Max URLs per sitemap"
        default: "200"
      max_workers:
        description: "Parallel requests per job (recommended 2-4 for Cloudflare)"
        default: "3"
      request_delay:
        description: "Base delay between requests in seconds"
        default: "1.5"

env:
  PYTHON_VERSION: '3.11'

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
      total_sitemaps_found: ${{ steps.discovery.outputs.total_sitemaps }}
    steps:
      - name: Discover sitemap structure
        id: discovery
        run: |
          URL="${{ github.event.inputs.url }}"
          
          # Install dependencies
          sudo apt-get update
          sudo apt-get install -y curl
          
          echo "Discovering sitemap structure for: $URL"
          
          # Try multiple sitemap locations
          SITEMAP_LOCATIONS=(
            "$URL/sitemap.xml"
            "$URL/sitemap_index.xml"
            "$URL/sitemaps/sitemap.xml"
            "$URL/sitemap/sitemap.xml"
          )
          
          for sitemap in "${SITEMAP_LOCATIONS[@]}"; do
            echo "Trying: $sitemap"
            if curl -s -f -L -H "User-Agent: Mozilla/5.0" "$sitemap" | grep -q "<\?xml"; then
              echo "Found valid sitemap at: $sitemap"
              COUNT=$(curl -s -L -H "User-Agent: Mozilla/5.0" "$sitemap" | grep -c "<loc>" || echo "0")
              echo "total_sitemaps=$COUNT" >> $GITHUB_OUTPUT
              exit 0
            fi
          done
          
          # Fallback
          echo "Could not auto-detect sitemap, using default"
          echo "total_sitemaps=20" >> $GITHUB_OUTPUT

      - name: Generate job matrix
        id: matrix
        run: |
          DISCOVERED=${{ steps.discovery.outputs.total_sitemaps }}
          USER_INPUT=${{ github.event.inputs.total_sitemaps }}
          
          if [ "$USER_INPUT" -gt 0 ]; then
            TOTAL=$USER_INPUT
          elif [ "$DISCOVERED" -gt 0 ]; then
            TOTAL=$DISCOVERED
          else
            TOTAL=20
          fi
          
          PER_JOB=${{ github.event.inputs.sitemaps_per_job }}
          URL="${{ github.event.inputs.url }}"
          REQ_DELAY="${{ github.event.inputs.request_delay }}"
          
          JOBS=$(( (TOTAL + PER_JOB - 1) / PER_JOB ))
          
          MATRIX="["
          for ((i=0;i<JOBS;i++)); do
            OFFSET=$(( i * PER_JOB ))
            MATRIX+="{\"offset\":$OFFSET,\"limit\":$PER_JOB,\"url\":\"$URL\",\"request_delay\":\"$REQ_DELAY\"},"
          done
          MATRIX="${MATRIX%,}]"
          
          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
          echo "Total: $TOTAL sitemaps, $JOBS jobs"

  scrape:
    needs: plan
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}
    timeout-minutes: 120

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y pkg-config libssl-dev libcurl4-openssl-dev

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install cloudscraper curl-cffi requests lxml

      - name: Run Cloudflare-resistant scraper
        env:
          CURR_URL: ${{ matrix.url }}
          SITEMAP_OFFSET: ${{ matrix.offset }}
          MAX_SITEMAPS: ${{ matrix.limit }}
          MAX_URLS_PER_SITEMAP: ${{ github.event.inputs.urls_per_sitemap }}
          MAX_WORKERS: ${{ github.event.inputs.max_workers }}
          REQUEST_DELAY_BASE: ${{ matrix.request_delay }}
        run: |
          python shopify-scrapper/shopifyscrap-cloudflare.py

      - name: Upload chunk artifact
        uses: actions/upload-artifact@v4
        with:
          name: chunk_${{ matrix.offset }}
          path: products_chunk_*.csv
          retention-days: 1

  merge:
    needs: scrape
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Download all chunk artifacts
        uses: actions/download-artifact@v4
        with:
          path: chunks
          pattern: chunk_*
          merge-multiple: false

      - name: Merge CSV chunks
        run: |
          echo "Merging CSV files..."
          
          # Find all CSV files
          find chunks -name "*.csv" -type f | sort > csv_files.txt
          
          if [ ! -s csv_files.txt ]; then
            echo "No CSV files found to merge!"
            exit 1
          fi
          
          FIRST_FILE=$(head -n 1 csv_files.txt)
          
          # Create merged file with header from first file
          head -n 1 "$FIRST_FILE" > products_full.csv
          
          # Append data from all files (skip headers)
          while IFS= read -r csv_file; do
            echo "Adding: $csv_file"
            tail -n +2 "$csv_file" >> products_full.csv
          done < csv_files.txt
          
          ROW_COUNT=$(tail -n +2 products_full.csv | wc -l | tr -d ' ')
          echo "Merged $ROW_COUNT product rows"

      - name: Generate output filename
        id: filename
        run: |
          SITE=$(echo "${{ github.event.inputs.url }}" | sed -E 's|https?://||;s|/.*||')
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          echo "filename=${SITE}_products_${TIMESTAMP}.csv" >> $GITHUB_OUTPUT
          echo "Output file: ${SITE}_products_${TIMESTAMP}.csv"

      - name: Upload to FTP (Optional)
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_PATH: ${{ secrets.FTP_PATH }}
          OUTPUT_FILE: ${{ steps.filename.outputs.filename }}
        run: |
          sudo apt-get install -y lftp
          echo "Uploading to FTP..."
          lftp -u "$FTP_USER","$FTP_PASS" "$FTP_HOST" <<EOF
          set ssl:verify-certificate no
          mkdir -p $FTP_PATH
          cd $FTP_PATH
          put products_full.csv -o $OUTPUT_FILE
          ls -la $OUTPUT_FILE
          bye
          EOF
          echo "FTP upload complete"

      - name: Upload merged CSV as artifact
        uses: actions/upload-artifact@v4
        with:
          name: merged_products
          path: products_full.csv
          retention-days: 7

      - name: Create summary report
        run: |
          TOTAL_CHUNKS=$(find chunks -mindepth 1 -maxdepth 1 -type d | wc -l)
          TOTAL_ROWS=$(tail -n +2 products_full.csv 2>/dev/null | wc -l | tr -d ' ' || echo "0")
          
          echo "## Scraping Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Target URL**: ${{ github.event.inputs.url }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Chunks**: $TOTAL_CHUNKS" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Products**: $TOTAL_ROWS" >> $GITHUB_STEP_SUMMARY
          echo "- **Output File**: ${{ steps.filename.outputs.filename }}" >> $GITHUB_STEP_SUMMARY
          
          if [ $TOTAL_ROWS -gt 0 ]; then
            echo "✅ Scraping completed successfully!" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Warning: No products were scraped" >> $GITHUB_STEP_SUMMARY
          fi