name: Cymax Scraper with FlareSolverr (URL Chunks)

on:
  workflow_dispatch:
    inputs:
      url:
        description: "Base site URL"
        required: true
        default: "https://www.cymax.com"
      total_sitemaps:
        description: "How many top-level sitemaps to scan in planning"
        default: "13"
      urls_per_sitemap:
        description: "Max URLs to include from one sitemap (0 = all)"
        default: "0"
      chunk_size:
        description: "URLs per scrape job"
        default: "50000"
      max_workers:
        description: "Parallel requests per scrape job"
        default: "6"
      flaresolverr_instances:
        description: "FlareSolverr instances per job (0 = same as max_workers)"
        default: "0"
      request_delay:
        description: "Delay between requests in scraper"
        default: "0"

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.generate_matrix.outputs.matrix }}
      site_name: ${{ steps.get_site_name.outputs.site_name }}
      total_urls: ${{ steps.generate_matrix.outputs.total_urls }}
      chunk_count: ${{ steps.generate_matrix.outputs.chunk_count }}
    steps:
      - uses: actions/checkout@v4

      - name: Get site name from URL
        id: get_site_name
        run: |
          URL="${{ github.event.inputs.url }}"
          SITE_NAME=$(echo "$URL" | sed -E 's|https?://||;s|www\.||;s|\.[a-zA-Z]+(/.*)?$||;s|/.*||')
          echo "site_name=$SITE_NAME" >> $GITHUB_OUTPUT
          echo "Site name extracted: $SITE_NAME"

      - name: Start FlareSolverr pool
        id: flaresolverr_pool
        run: |
          INSTANCES="${{ github.event.inputs.flaresolverr_instances }}"
          MAX_WORKERS="${{ github.event.inputs.max_workers }}"
          if [ -z "$INSTANCES" ] || [ "$INSTANCES" = "0" ]; then
            INSTANCES="$MAX_WORKERS"
          fi
          if [ "$INSTANCES" -lt 1 ]; then
            INSTANCES=1
          fi

          BASE_PORT=8191
          URLS=""
          echo "Starting $INSTANCES FlareSolverr instances..."
          for ((i=0; i<INSTANCES; i++)); do
            PORT=$((BASE_PORT + i))
            NAME="flaresolverr_${PORT}"
            docker run -d --name "$NAME" --cap-add=SYS_ADMIN -p "${PORT}:8191" ghcr.io/flaresolverr/flaresolverr:latest >/dev/null
          done

          echo "Waiting for all FlareSolverr instances..."
          for ((i=0; i<INSTANCES; i++)); do
            PORT=$((BASE_PORT + i))
            NAME="flaresolverr_${PORT}"
            URL="http://localhost:${PORT}/v1"
            READY=0
            for attempt in {1..45}; do
              if curl -s -f -X POST "$URL" -H "Content-Type: application/json" -d '{"cmd":"sessions.list"}' > /dev/null 2>&1; then
                READY=1
                echo "✓ $NAME ready"
                break
              fi
              sleep 2
            done
            if [ "$READY" -ne 1 ]; then
              echo "❌ $NAME failed to start"
              docker logs "$NAME" || true
              exit 1
            fi
            if [ -z "$URLS" ]; then URLS="$URL"; else URLS="$URLS,$URL"; fi
          done

          echo "instances=$INSTANCES" >> $GITHUB_OUTPUT
          echo "urls=$URLS" >> $GITHUB_OUTPUT
          echo "first_url=http://localhost:8191/v1" >> $GITHUB_OUTPUT

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Generate URL chunks matrix
        id: generate_matrix
        env:
          CURR_URL: ${{ github.event.inputs.url }}
          MAX_SITEMAPS: ${{ github.event.inputs.total_sitemaps }}
          MAX_URLS_PER_SITEMAP: ${{ github.event.inputs.urls_per_sitemap }}
          CHUNK_SIZE: ${{ github.event.inputs.chunk_size }}
          MAX_WORKERS: ${{ github.event.inputs.max_workers }}
          FLARESOLVERR_URL: ${{ steps.flaresolverr_pool.outputs.first_url }}
          FLARESOLVERR_URLS: ${{ steps.flaresolverr_pool.outputs.urls }}
        run: |
          python cymax/generate_chunks.py
          test -f cymax_chunk_urls.txt
          wc -l cymax_chunk_urls.txt

      - name: Upload URL list artifact
        uses: actions/upload-artifact@v4
        with:
          name: cymax_url_list
          path: cymax_chunk_urls.txt
          if-no-files-found: error

      - name: Stop FlareSolverr pool
        if: always()
        run: |
          docker ps -a --format '{{.Names}}' | grep '^flaresolverr_' | xargs -r docker rm -f

  scrape:
    needs: plan
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}
    steps:
      - uses: actions/checkout@v4

      - name: Download URL list
        uses: actions/download-artifact@v4
        with:
          name: cymax_url_list
          path: .

      - name: Start FlareSolverr pool
        id: flaresolverr_pool
        run: |
          INSTANCES="${{ github.event.inputs.flaresolverr_instances }}"
          MAX_WORKERS="${{ github.event.inputs.max_workers }}"
          if [ -z "$INSTANCES" ] || [ "$INSTANCES" = "0" ]; then
            INSTANCES="$MAX_WORKERS"
          fi
          if [ "$INSTANCES" -lt 1 ]; then
            INSTANCES=1
          fi
          BASE_PORT=8191
          URLS=""
          echo "Starting $INSTANCES FlareSolverr instances..."
          for ((i=0; i<INSTANCES; i++)); do
            PORT=$((BASE_PORT + i))
            NAME="flaresolverr_${PORT}"
            docker run -d --name "$NAME" --cap-add=SYS_ADMIN -p "${PORT}:8191" ghcr.io/flaresolverr/flaresolverr:latest >/dev/null
          done
          echo "Waiting for all FlareSolverr instances..."
          for ((i=0; i<INSTANCES; i++)); do
            PORT=$((BASE_PORT + i))
            NAME="flaresolverr_${PORT}"
            URL="http://localhost:${PORT}/v1"
            READY=0
            for attempt in {1..45}; do
              if curl -s -f -X POST "$URL" -H "Content-Type: application/json" -d '{"cmd":"sessions.list"}' > /dev/null 2>&1; then
                READY=1
                break
              fi
              sleep 2
            done
            if [ "$READY" -ne 1 ]; then
              echo "❌ $NAME failed to start"
              docker logs "$NAME" || true
              exit 1
            fi
            if [ -z "$URLS" ]; then URLS="$URL"; else URLS="$URLS,$URL"; fi
          done
          echo "instances=$INSTANCES" >> $GITHUB_OUTPUT
          echo "urls=$URLS" >> $GITHUB_OUTPUT
          echo "first_url=http://localhost:8191/v1" >> $GITHUB_OUTPUT

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml

      - name: Run Cymax scraper chunk
        id: scrape_run
        continue-on-error: true
        env:
          CURR_URL: ${{ github.event.inputs.url }}
          PRODUCT_URLS_FILE: cymax_chunk_urls.txt
          URL_OFFSET: ${{ matrix.offset }}
          URL_LIMIT: ${{ matrix.limit }}
          CHUNK_ID: ${{ matrix.chunk_id }}
          MAX_PRODUCTS: 0
          MAX_WORKERS: ${{ github.event.inputs.max_workers }}
          REQUEST_DELAY: ${{ github.event.inputs.request_delay }}
          FLARESOLVERR_URL: ${{ steps.flaresolverr_pool.outputs.first_url }}
          FLARESOLVERR_URLS: ${{ steps.flaresolverr_pool.outputs.urls }}
        run: |
          echo "Chunk ID: $CHUNK_ID"
          echo "URL_OFFSET: $URL_OFFSET"
          echo "URL_LIMIT: $URL_LIMIT"
          python cymax/cymax.py || echo "Scraper failed with exit code $?"
          ls -la cymax_products_*.csv 2>/dev/null || echo "No CSV files generated"

      - name: Stop FlareSolverr pool
        if: always()
        run: |
          docker ps -a --format '{{.Names}}' | grep '^flaresolverr_' | xargs -r docker rm -f

      - name: Upload chunk artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: chunk_${{ matrix.chunk_id }}
          path: cymax_products_*.csv
          if-no-files-found: warn

  merge:
    needs: [plan, scrape]
    runs-on: ubuntu-latest
    if: always() && !cancelled()
    steps:
      - name: Download all chunks
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          path: chunks
          pattern: chunk_*
          merge-multiple: true

      - name: Check for CSV files
        id: check_files
        run: |
          CSV_FILES=$(find chunks -name "cymax_products_*.csv" 2>/dev/null || echo "")
          if [ -z "$CSV_FILES" ]; then
            echo "has_files=false" >> $GITHUB_OUTPUT
            echo "ERROR: No CSV files found in artifacts!"
            exit 0
          else
            echo "has_files=true" >> $GITHUB_OUTPUT
            CHUNK_COUNT=$(echo "$CSV_FILES" | wc -l)
            echo "chunk_count=$CHUNK_COUNT" >> $GITHUB_OUTPUT
          fi

      - name: Merge CSV chunks
        id: merge_csv
        if: steps.check_files.outputs.has_files == 'true'
        run: |
          CSV_FILES=$(find chunks -name "cymax_products_*.csv")
          FIRST_FILE=$(echo "$CSV_FILES" | head -n 1)
          head -n 1 "$FIRST_FILE" > cymax_products_full.csv
          FILE_COUNT=0
          for f in $CSV_FILES; do
            if [ -f "$f" ]; then
              FILE_COUNT=$((FILE_COUNT + 1))
              tail -n +2 "$f" 2>/dev/null | sed '/^$/d' >> cymax_products_full.csv
            fi
          done
          FINAL_ROWS=$(tail -n +2 cymax_products_full.csv 2>/dev/null | wc -l || echo "0")
          echo "file_count=$FILE_COUNT" >> $GITHUB_OUTPUT
          echo "final_rows=$FINAL_ROWS" >> $GITHUB_OUTPUT

      - name: Build output filename
        id: meta
        run: |
          SITE_NAME="${{ needs.plan.outputs.site_name }}"
          if [ -z "$SITE_NAME" ]; then SITE_NAME="cymax"; fi
          DATE=$(date +%F_%H%M%S)
          echo "name=${SITE_NAME}_${DATE}.csv" >> $GITHUB_OUTPUT

      - name: Upload to FTP (if merge succeeded)
        if: steps.merge_csv.outputs.final_rows > 0
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_BASE_DIR: ${{ secrets.FTP_PATH }}
          FILE: ${{ steps.meta.outputs.name }}
        run: |
          if [ -z "$FTP_HOST" ] || [ -z "$FTP_USER" ] || [ -z "$FTP_PASS" ]; then
            echo "FTP credentials not configured. Skipping FTP upload."
            exit 0
          fi
          sudo apt-get update && sudo apt-get install -y lftp
          lftp -u "$FTP_USER","$FTP_PASS" "$FTP_HOST" <<EOF
          set ftp:ssl-allow no
          set cmd:fail-exit yes
          cd $FTP_BASE_DIR
          put cymax_products_full.csv -o $FILE
          bye
          EOF
          echo "Upload completed: $FILE"

      - name: Archive final results
        if: steps.merge_csv.outputs.final_rows > 0
        uses: actions/upload-artifact@v4
        with:
          name: cymax_final_results
          path: cymax_products_full.csv
          retention-days: 7
